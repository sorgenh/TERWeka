\documentclass{article}

\usepackage[francais]{babel}
\def\printlandscape{\special{landscape}}    % Works with dvips.
%\usepackage{pstricks,pst-node,pst-tree}
%\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{fancybox} % for shadow and Bitemize
\usepackage{alltt}
\usepackage{graphicx}
%\usepackage{caption} 


%\usepackage{epsfig}
\usepackage{fullpage}
%\usepackage{fancyhdr}
%\usepackage{moreverb}
%\usepackage{xspace}
\usepackage[colorlinks,hyperindex,bookmarks,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{array,multirow,makecell}
\setcellgapes{1pt}
\makegapedcells
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash }b{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash }b{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash }b{#1}}
\usepackage{wrapfig}
\usepackage{epsf}
\usepackage{framed}

\usepackage{fancyvrb}
\usepackage{xcolor}
\definecolor{Zgris}{rgb}{0.87,0.85,0.85}

\newsavebox{\BBbox}

\newenvironment{DDbox}[1]{
	\begin{lrbox}{\BBbox}\begin{minipage}{\linewidth}}
		{\end{minipage}\end{lrbox}\noindent\colorbox{Zgris}{\usebox{\BBbox}} \\
		[.5cm]}

\title{Rapport du TER GMIN401 :\\ \textbf{Intégration et optimisation d’algorithmes de classifications supervisées pour Weka}}

\author{Par : ALIJATE Mehdi - NEGROS Hadrien - TURKI Batoul}

\date{31 Janvier 2014}

         
\begin{document}

\maketitle
\tableofcontents

\begin{abstract}
	
Ce sujet vise à intégrer et à optimiser des algorithmes de classifications supervisées de documents dans la suite logiciel WEKA. Ces algorithmes sont issus de travaux de recherche menés récemment au sein du LIRMM.
\end{abstract}

\newpage 
%-----------------------------------------------------------
\section{Introduction}\label{sec:intro}

La classification de documents est le mécanisme consistant à classer automatiquement des ressources la classe prédéfinie lui correspondant le mieux.\\
Plusieurs formes de classification existent (par genre, par opinion, par thème...etc), et se font via des algorithmes de classifications spécifiques. Ceux-ci se basent sur des méthodes principalement numériques (probabilistes), avec des algorithmes utilisant les mathématiques ou basés sur la recherche d'information. \\
Ce TER vise justement à intégrer des algorithmes de classifications supervisées de documents dans la suite logiciel WEKA\footnote{\href{http://www.cs.waikato.ac.nz/~ml/weka/}{Weka est une suite populaire de logiciels d'apprentissage automatique. Écrite en Java, développée à l'université de Waikato, Nouvelle-Zélande. Weka est un Logiciel libre disponible sous la Licence publique générale GNU.}}, se basant sur un nouveau modèle de classification à partir d'un faible nombre de document, intégrant de nouvelles pondérations adaptées.\\
Tout d'abord, il faudra explorer l'API de WEKA, pour prendre en main du code source, la maniabilité des classes et explorer une méthode d'ajout d'un algorithme de classification. Ensuite, nous nous pencherons sur le développement des différentes classes en établissant une méthodologie concrétisant le travail mené au laboratoire du LIRMM, s'en suivra une phase d'intégration et de tests.\\

%-----------------------------------------------------------

\section{Exploration de WEKA}
Après la réunion du 24/01/14, nous avons établi un plan de travail pour bien mener et répartir les tâches de ce TER. Il a été décidé de le diviser en trois grandes parties. La première, qui est décrite ci-dessous consiste à explorer et prendre en main l'API de WEKA, afin de pouvoir y rajouter les algorithmes que l'on aura développé lors de la deuxième partie, et qui seront testés et intégrés lors de la troisième.
\subsection{L'API Weka/Sources avec Eclipse}
Pour explorer l'API, nous nous sommes aidés de l'IDE Eclipse, qui permet facilement parcourir les sources d'une librairie externe. Après avoir étudié l'arborescence des classes de l'API, nous avons pu cibler les différentes classes et méthodes qui nous intéressent, et étudié leurs fonctionnement. Nous nous sommes aidé de ce wiki \footnote{\href{http://weka.wikispaces.com/}{http://weka.wikispaces.com/}}.

\subsection{L'utilisation des classes}
Une fois familiarisés avec l'API Weka, on a creusé un peu plus du côté des classes qui pourraient nous être utiles pour ce TER. Il s'agit des certaines classes présentes dans le package{\scriptsize { \normalsize "weka.classifiers"}}. En effet, notre but étant d'intégrer des algorithmes de classification, il est utile de savoir comment tournent les algorithmes de classifications, leur paramétrage et l'architecture pour organiser les ressources pour ces derniers.\\
Quelques tests ont été menés notamment pour bayes naif multinomial, que nous avons fait tourné sur différentes données, et avec différentes options.
\subsection{Ajout d'un algorithme dans Weka}
Après avoir étudié en détail la classe \textit{NaiveBayesMultinomial}, nous avons remarqué que le calcul des pondérations (dans l'implémentation de Weka, seul la mesure intra-classe Tf est utilisée) se fait dans la méthode \textbf{buildClassifier}. Nous allons donc créer une sous classe de \textit{NaiveBayesMultinomial}, contenant une méthode surchargeant \textbf{buildClassifier} dans laquelle nous calculerons toutes les pondérations supplémentaires.\\
Une fois tout cela creusé et vu en détails, il faudra intégrer l'algorithme dans l'écosystème de Weka, c'est à dire, pour le rendre disponible dans l'Explorateur, expérimentateur, etc . 
Weka prend en charge les classes dérivées dans le package, ceci est géré par le \textit{GenericPropertiesCreator}. Il faudra donc dire à Weka où trouver notre nouveau classificateur et il s'occupera de l'afficher dans la \textit{GenericObjectEditor}.\\
Nous y reviendrons plus en détails lors de la troisième étape de notre TER : L'intégration des algorithmes dans WEKA.
%-----------------------------------------------------------
\section{De nouvelles méthodes de classification}
Dans cette partie, nous allons vous présenter les différentes pondérations que nous allons utiliser pour construire nos classifieurs.
Nous allons d'abord définir les mesures intra-classe inspirées du TF-IDF, puis les mesures inter-classe développées au \textit{LIRMM}.
Ces mesures vont nous permettre de définir si un terme (un élément d'un document) est plus ou moins représentatif de la classe.\\Toutes ces mesures ont étés définies dans l'article \textit{De nouvelles pondérations adaptées à la classification de petits volumes de données textuelles. }\cite{RNTIB}.
\subsection{Pondérations intra-classe}
Les pondérations que nous définissons ci-dessous sont dites \textbf{intra-classe} car les différentes valeurs que nous utilisons pour les
calculer sont dépendante d'une classe.
\subsubsection*{intra-classe document}
Cette mesure dépend du nombre de documents contenant le terme dans la classe.
  \[ inner\mbox{-}weight_{ij}^{Df} = \frac{DF_{ti}^j}{|d_{j}|}\]
  
      Avec:
  \begin{itemize}
  	\item $DF_{ti}^j$: Nombre de documents contenant le terme $t_i$ dans la classe $C_j$	
  	\item $|d_{j}|$: Nombre de documents dans $C_j$	
    \end{itemize}
\subsubsection*{intra-classe terme}
Cette mesure dépend du nombre d'occurrences du terme dans la classe.
\[   inner\mbox{-}weight_{ij}^{Tf} = \frac{TF_{ti}^j}{|n_{j}|}\]
  Avec:
\begin{itemize}
	\item $TF_{ti}^j$: Nombre d'occurrences du terme $t_i$ dans la classe $C_j$	
	\item $|n_{j}|$: Nombre de termes total dans la classe $C_j$	
  \end{itemize}
\subsection{Pondérations inter-classe}
Les pondérations inter-classes en revanche utilisent des valeurs calculées à partir de l'ensemble du corpus (depuis les classes extérieures à celle qui nous intéresse).
\subsubsection*{inter-classe terme}
Cette mesure dépend du nombre de classes contenant le terme.
\[inter\mbox{-}weight_{ij}^{class} = log_2 \frac{|C|}{C_{ti}}\]
Avec:
\begin{itemize}
	\item $|C|$: Nombre de classes			
	\item $C_{ti}$: Nombre de classes contenant le terme $t_i$
  \end{itemize}
\subsubsection*{inter-classe document}
Cette mesure dépend du nombre de documents extérieurs à la classe contenant le terme.
\[ inter\mbox{-}weight_{ij}^{doc} = log_2 \frac{|d \notin{C_j}|+1}{|d:t_i \notin{C_j}|+1}= log_2 \frac{|d|-|d \in{C_j}|+1}{|d:t_i|-|d:t_i \in{C_j}|+1}\]
 Avec:
\begin{itemize}
	\item $|d \notin  {C_j}|$: Nombre de documents n'appartenant pas à la classe $C_j$
	\item $|d:t_i \notin {C_j}|$: Nombre de documents n'appartenant pas à la classe $C_j$ qui contient $t_i$ 
	\item $|d|$: Nombre de documents dans l'ensemble des classes
	\item $|d \in  {C_j}|$: Nombre de documents de la classe $C_j$
	\item $|d:t_i|$: Nombre de documents dans l'ensemble des classes contenant le terme $t_i$ 
	\item $|d:t_i \in {C_j}|$: Nombre de documents de la classe $C_j$ qui contient $t_i$ 
	\item En ajoutant 1, permet de prévenir le cas où $t_i$ est uniquement utilisé dans $C_j$ (quand $|d:t_i \notin{C_j}|={|d:t_i|-|d:t_i \in{C_j}|} = 0$)
  \end{itemize}
\subsection{Algorithmes de classifications}
Un algorithme de classification permet de calculer la probabilité de l'appartenance d'un document aux différentes classes du corpus, et donc de l'affecter à la plus probable. Nous allons implémenter un classifieur \textit{Naive Bayes} et \textit{Class-Feature-Centroid}\cite{RNTIB} en utilisant les mesures définies plus haut. Pour calculer la probabilité $w_{ij}$ d'un terme $i$ dans une classe $j$, nous allons combiner les différentes pondérations de 4 façons:
\begin{itemize}
\item $w_{ij}^{Tf-Class}$=$inner$-$weight_{ij}^{Tf}$ $\times$ $inter$-$weight_{ij}^{class}$
%=$ \frac{TF_{ti}^j}{|n_{j}|}$ x $log_2 \frac{|C|}{CF_{ti}}$
\item $w_{ij}^{Df-Class}$=$inner$-$weight_{ij}^{Df}$ $\times$ $inter$-$weight_{ij}^{class}$
%=$\frac{DF_{ti}^j}{|d_{j}|}$ x $log_2 \frac{|C|}{CF_{ti}}$
\item $w_{ij}^{Tf-Doc}$=$inner$-$weight_{ij}^{Tf}$ $\times$ $inter$-$weight_{ij}^{doc}$
%=$ \frac{TF_{ti}^j}{|n_{j}|}$ x $log_2 \frac{|d|-|d \in{C_j}|+1}{|d:t_i|-|d:t_i \in{C_j}|+1}$
\item $w_{ij}^{Df-Doc}$=$inner$-$weight_{ij}^{Df}$ $\times$ $inter$-$weight_{ij}^{doc}$
%=$\frac{DF_{ti}^j}{|d_{j}|}$ x $log_2 \frac{|d|-|d \in{C_j}|+1}{|d:t_i|-|d:t_i \in{C_j}|+1}$
\end{itemize}
Nous allons aussi mettre en place une combinaison de ces mesures dépendante de deux paramètres $\alpha$ et $\beta$:
\[ w_{ij}^{ \alpha \beta}=(\alpha \times innerweight_{ij}^{Tf} + (1-\alpha)\times innerweight_{ij}^{Df} )\times(\beta \times interweight_{ij}^{class} + (1-\beta) \times interweight_{ij}^{doc} )\]
On remarque qu'en faisant varier $\alpha$ et $\beta$, on peut retrouver les quatre premières formules.
%-----------------------------------------------------------



\section{Développement des différentes classes}

\subsection{Méthodologie}
//TODO
\subsection{Extension de Naive Bayes Multinomial}
//TODO
\subsection{Class-Feature-Centroide}
//TODO
%-----------------------------------------------------------



\section{Intégration et tests}
\subsection{Intégration dans l'écosystème de Weka}
//TODO
\subsection{Tests}
//TODO
\subsection{Résultats}
//TODO

%-----------------------------------------------------------


\section{Discussion et Conclusion }
//TODO
%-----------------------------------------------------------

\newpage
\section{Sources}
//TODO

%-----------------------------------------------------------
\bibliographystyle{plain}
\bibliography{biblio}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: utf-8
%%% End:
